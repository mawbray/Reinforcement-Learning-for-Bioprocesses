# Importing (only for Monte Carlo Learner)
import numpy as np
import pandas as pd
import scipy.integrate as scp

import matplotlib.pyplot as plt

import numpy.random as rnd
from scipy.spatial.distance import cdist

from scipy.optimize import minimize
eps  = np.finfo(float).eps
import csv





####################### -- Defining Parameters and Designing Experiement -- #########################
#investigating steps_, alpha and discount factor for 1,000,000 epochs of training 
# Model definitions: parameters, steps, tf, x0  
      
p        = {'u_m' : 0.0923*0.62, 'K_N' : 393.10, 'u_d' : 0.01, 'Y_nx' : 504.49}
steps_   = np.array([10])
tf       = 16.*24
x0       = np.array([0.5,150.0])
modulus  = np.array([0.05, 10])
state_UB = np.array([5, 1000])

# Agent definitions: num_actions, eps_prob, alpha, discount
num_actions = 15

disc1 = np.array([0.55, 0.65, 0.75, 0.85, 0.95, 0.99])
disc2 = np.array([0.75, 0.85, 0.95, 0.99])
xi_ = np.array([3])                # Epsilon greedy definitions 

# Experiment defintions: env, agent, controls, episodes
controls = np.linspace(0,7,num_actions)
episodes_train = 1000000
episodes_valid = 1000
reward_training = np.zeros((episodes_train, xi_.shape[0], disc1.shape[0], disc2.shape[0]))
reward_validation = np.zeros((episodes_valid, xi_.shape[0], disc1.shape[0], disc2.shape[0]))

#running experiement
for i in range(0, xi_.shape[0]):
  for j in range(0, disc1.shape[0]):    
    for k in range(0,disc2.shape[0]): 
      #run training 
      env = Model_env(p, steps_, tf, x0, modulus)
      agent1 = greedye_MCL(num_actions, modulus, state_UB, disc1[j], disc2[k], steps_)
      experiment = Experiment(env, agent1, controls, episodes_train, xi_[i])               
      reward_training[:,i,j,k], d, site_log = experiment.simulation()
      agent2 = Greedye_MCLearned(num_actions, d, steps_)
      exp_done = Experiment_Done(env, agent2, controls, episodes_valid, i, j, k, 0)
      reward_validation[:,i,j,k] = exp_done.simulation()
 
