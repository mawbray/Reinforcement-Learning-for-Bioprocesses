# -*- coding: utf-8 -*-
"""
Created on Tue Nov  5 09:34:05 2019
@author: Max Mowbray - University of Manchester, Manchester, United Kingdom
"""
#import pylab
import numpy as np
import pandas as pd
import scipy.integrate as scp
#from pylab import *
import matplotlib.pyplot as plt
#import csv
#import os
#import sys
#import copy
import numpy.random as rnd
from scipy.spatial.distance import cdist
#!pip install sobol_seq
#import sobol_seq
from scipy.optimize import minimize
eps  = np.finfo(float).eps
import csv

############ Defining Environment ##############

class Model_env: 
    
    # --- initializing model --- #
    def __init__(self, parameters, steps, tf, x0, modulus):
        
        # Object variable definitions
        self.parameters, self.steps = parameters, steps
        self.x0, self.dt, self.tf      = x0, tf/steps, tf                
        self.modulus                   = modulus          # two column array [biomass nitrate ]
        
    # --- dynamic model definition --- #    
    # model takes state and action of previous time step and integrates -- definition of ODE system at time, t
    def model(self, t, state, control):
        # internal definitions
        params = self.parameters
        FCn   = control
                
        # state vector
        Cx  = state[0]
        Cn  = state[1]
        
        # parameters
        u_m  = params['u_m']; K_N  = params['K_N'];
        u_d  = params['u_d']; Y_nx = params['Y_nx'];
        
        # algebraic equations
        
        # variable rate equations
        dev_Cx  = u_m * Cx * Cn/(Cn+K_N) - u_d*Cx**2
        dev_Cn  = - Y_nx * u_m * Cx * Cn/(Cn+K_N) + FCn
        
        return np.array([dev_Cx, dev_Cn],dtype='float64')
    
    def discrete_env(self, state):
        # discretisation of the system, with introduction of stochasticity in terms of modulus
        modulus = self.modulus
        
            
        resid = state % modulus
        resid = resid/modulus
        UB = 1 - resid
        draw =  np.random.uniform(0,1,2)

        for i in range(state.shape[0]):
            if draw[i] < UB[i]:
              state[i] = state[i] - resid[i] * modulus[i]
            else:
              state[i] = state[i] - resid[i] * modulus[i] + modulus[i]
        
        # fixes for representation 
        #Nitrate fix
        if state[1] < 0:
          state[1] = 0
        elif state[0] < 0:
            state[0] = 0
        
        #Biomass fix
        f = str(self.modulus[0])
        decimal = f[::-1].find('.')  
        state[0] = np.round(state[0], decimal)
        f1 = str(self.modulus[1])
        decimal1 = f1[::-1].find('.')  
        state[0] = np.round(state[0], decimal1)

        if state[0] == eps:
            state[0] = 0
        if state[1] == eps:
            state[1] = 0
        
        return state

    def reward(self, state):
      reward = 100*state[-1][0] - state[-1][1]              # objective function 1
      return reward

        
################# --- Training simulation --- #####################
class Experiment(object):
    def __init__(self, env, agent, controls, episodes,xi):
      self.env , self.agent,             = env, agent 
      self.controls, self.episodes, self.xi      = controls, episodes, xi

    def eps_prob(self,ei,episodes):
        if self.xi == int(1):
            F = 0.1
            G = -np.log(0.1)*F*episodes     # =no of episodes until behave =0.1
            if ei < G:
                behave = np.exp(-ei/(episodes*F))
            else:
                behave = 0.1
        elif self.xi == int(2):
            F = 0.2
            G = -np.log(0.1)*F*episodes     # =no of episodes until behave =0.1
            if ei < G:
                behave = np.exp(-ei/(episodes*F))
            else:
                behave = 0.1
        elif self.xi == int(3):
            F = 0.3
            G = -np.log(0.1)*F*episodes     # =no of episodes until behave =0.1
            if ei < G:
                behave = np.exp(-ei/(episodes*F))
            else:
                behave = 0.1
        elif self.xi == int(4):
            F = 0.4
            G = -np.log(0.1)*F*episodes     # =no of episodes until behave =0.1
            if ei < G:
                behave = np.exp(-ei/(episodes*F))
            else:
                behave = 0.1
        elif self.xi == int(5):
            F = 0.5
            G = -np.log(0.1)*F*episodes     # =no of episodes until behave =0.1
            if ei < G:
                behave = np.exp(-ei/(episodes*F))
            else:
                behave = 0.1
        elif self.xi == int(6):
            F = 0.05
            G = -np.log(0.1)*F*episodes     # =no of episodes until behave =0.1
            if ei < G:
                behave = np.exp(-ei/(episodes*F))
            else:
                behave = 0.1
        elif self.xi == int(7):
            F = 0.01
            G = -np.log(0.1)*F*episodes     # =no of episodes until behave =0.1
            if ei < G:
                behave = np.exp(-ei/(episodes*F))
            else:
                behave = 0.1
        else: behave = 1                    # behave randomly all the time
        return behave      
            
       
    def simulation(self):
      # Simulation takes environment and simulates, next iteration and outputs reward
      # internal definitions
      discrete_env = self.env.discrete_env
      dt, movements, x0   = self.env.dt, int(self.env.tf/float(self.env.dt)), self.env.x0
      model, ctrls = self.env.model, self.controls       #takes set of control options
      episodes = self.episodes

      # compile state and control trajectories
      xt = np.zeros((movements+1, x0.shape[0], episodes))
      tt = np.zeros((movements+1))
      c_hist = np.zeros((movements, episodes))
      ctrl = np.zeros((movements, episodes))
      reward = np.zeros((episodes))
      #plot_m = np.array([0, episodes-1])

      for ei in range(episodes):
        # initialize simulation
        current_state = x0
        xt[0,:,ei]  = current_state
        tt[0]    = 0.
        
        
        # define e greedy policy exploration
        #print(xi, ei, episodes)
        eps_prob = self.eps_prob(ei,episodes)
      
        # simulation
        for s in range(movements):
            action_indx  = self.agent.act(current_state, eps_prob, s)        # select control for this step from that possible
            ctrl[s,ei]   =  ctrls[action_indx]                               # find control action relevant to index from agent.act
            c_hist[s,ei] = action_indx                                       # storing control history for each epoch
            ode          = scp.ode(self.env.model)                           # define ode
            ode.set_integrator('lsoda', nsteps=3000)                         # define integrator
            ode.set_initial_value(current_state,dt)                          # set initial value
            ode.set_f_params(ctrl[s,ei])                                     # set control action
            current_state = list(ode.integrate(ode.t + dt))                  # integrate system
            current_state = discrete_env(np.array(current_state))
            xt[s+1,:,ei]  = current_state                                    # add current state Note: here we can add randomnes as: + RandomNormal noise
            tt[s+1]       = (s+1)*dt
        
        for i in [0, 0.2, 0.4, 0.6, 0.8]:
            if i == ei/episodes:
                print('Simulation is', i*100 , ' percent complete')
        
        #print('state variables =', xt[:,:,ei], 'control input =', ctrl[:,ei])

        reward[ei] = self.env.reward(xt[:,:,ei])
        # QLearning
        self.agent.MClearn(xt[:,:,ei], c_hist[:,ei], reward[ei])
        self.agent.SiteVisit(xt[:,:,ei])
        
        #checking training 
        #if ei in plot_m:
            #plot integration of system and state variables with time
            #print(f'Reward of run {ei} = ', reward[ei],f' Final Biomass Conc {ei} = ', xt[-1,0,ei], f' Final Nitrate Conc {ei} = ', xt[-1,1,ei])
            #plt.figure(figsize = (10,4))
            #plt.plot(tt, xt[:,0,ei]/np.max(xt[:,0,ei]), color = 'blue', label = 'biomass')
            #plt.plot(tt, xt[:,1,ei]/np.max(xt[:,1,ei]), color = 'orange', label = 'Nitrate Conc')
            #plt.plot(tt[:-1], ctrl[:,ei]/np.max(ctrl[:,ei]), 'green', label = 'Nitrate inflow / m3')
            #plt.ylabel(f'State_Variables{ei}')
            #plt.legend(loc='upper left')
            #plt.savefig(f"ControlAction_Train_epoch{ei}.svg")
            
      d = self.agent.learned()
      Site_log = self.agent.SitevisitLog()
      return reward, d, Site_log
      # plot and show learning process --> reward with episodes
      #for ei in range(episodes):
       # if reward[ei] < 0:
        #  reward[ei] = 0

      #plt.figure(figsize=(10,4))
      #plt.scatter(np.array(range(0,episodes)), reward/np.max(reward), color = "orange")
      #axes = plt.gca()
      #axes.set_ylim([0,1])
      #plt.show()
      
#################### --- Validation Simulation --- #####################
class Experiment_Done(object):
    def __init__(self, env, agent, controls, episodes, alp, disc1, disc2, eps):
      self.env , self.agent,             = env, agent 
      self.controls, self.episodes       = controls, episodes
      self.alp, self.disc1, self.disc2, self.eps      = alp, disc1, disc2, eps
       
    def simulation(self):
      # Simulation takes environment and simulates, next iteration and outputs reward
      # internal definitions
      discrete_env = self.env.discrete_env
      dt, movements, x0   = self.env.dt, int(self.env.tf/float(self.env.dt)), self.env.x0
      model, ctrls = self.env.model, self.controls       #takes set of control options
      episodes = self.episodes

      # compile state and control trajectories
      xt = np.zeros((movements+1, x0.shape[0], episodes))
      tt = np.zeros((movements+1))
      c_hist = np.zeros((movements, episodes))
      ctrl = np.zeros((movements, episodes))
      reward = np.zeros((episodes))
      plot_m = np.array([episodes-1])

      for ei in range(episodes):
        # initialize simulation
        current_state = x0
        xt[0,:,ei]  = current_state
        tt[0]    = 0.

        # define e greedy policy exploration
        eps_prob = self.eps        #act greedily
      
        # simulation
        for s in range(movements):
            action_indx = self.agent.act(current_state, eps_prob, s)        # select control for this step from that possible
            ctrl[s,ei]   =  ctrls[action_indx]                              # find control action relevant to index from agent.act
            c_hist[s,ei] = action_indx                                      # storing control history for each epoch
            ode       = scp.ode(self.env.model)                             # define ode
            ode.set_integrator('lsoda', nsteps=3000)                        # define integrator
            ode.set_initial_value(current_state,dt)                         # set initial value
            ode.set_f_params(ctrl[s,ei])                                    # set control action
            current_state = list(ode.integrate(ode.t + dt))                 # integrate system
            current_state = discrete_env(np.array(current_state))
            xt[s+1,:,ei]     = current_state                                # add current state Note: here we can add randomnes as: + RandomNormal noise
            tt[s+1]       = (s+1)*dt
        
        
        reward[ei] = self.env.reward(xt[:,:,ei])
         
        # output figure to demonstrate learning 
        if ei in plot_m:
            #plot integration of system and state variables with time
            #print(f'Reward of run {ei} = ', reward[ei],f' Final Biomass Conc {ei} = ', xt[-1,0,ei], f' Final Nitrate Conc {ei} = ', xt[-1,1,ei])
            plt.figure(figsize = (10,4))
            plt.plot(tt, xt[:,0,ei]/np.max(xt[:,0,ei]), color = 'blue', label = 'biomass')
            plt.plot(tt, xt[:,1,ei]/np.max(xt[:,1,ei]), color = 'orange', label = 'Nitrate Conc')
            plt.plot(tt[:-1], ctrl[:,ei]/np.max(ctrl[:,ei]), 'green', label = 'Nitrate inflow / m3')
            plt.ylabel(f'Normalised State Variables{ei}')
            plt.xlabel('Time (hours)')
            plt.legend(loc = 'upper left')
            plt.savefig(f"P:\\Documents\\Projects\\First Year\\RL-ContinuousBP\\MCarlo\\Table Look Up\\Multiple\\IncrementalMean_VariationalDeriv\\Discount_Exp\\ControlAction_Valid_epoch{ei}_xi{self.alp}_d1{self.disc1}_d2{self.disc2}_eps{self.eps}.svg")
            plt.close()
      return reward  
  
     
      
      # plot and show learning process --> reward with episodes
      #plt.figure(figsize=(10,4))
      #plt.scatter(np.array(range(0,episodes)), reward/np.max(reward), color = "orange")
      #axes = plt.gca()
      #axes.set_ylim([0,1])
      #plt.show()
      

####################### -- Defining Parameters and Designing Experiement -- #########################
#investigating steps_, alpha and discount factor for 500,000 episodes of training each
# Model definitions: parameters, steps, tf, x0  
      
p        = {'u_m' : 0.0923*0.62, 'K_N' : 393.10, 'u_d' : 0.01, 'Y_nx' : 504.49}
steps_   = np.array([10])
tf       = 16.*24
x0       = np.array([0.5,150.0])
modulus = np.array([0.05, 10])
state_UB = np.array([5, 1000])

# Agent definitions: num_actions, eps_prob, alpha, discount
num_actions = 15

disc1 = np.array([0.55, 0.65, 0.75, 0.85, 0.95, 0.99])
disc2 = np.array([0.75, 0.85, 0.95, 0.99])
xi_ = np.array([3])                # Epsilon greedy definitions 

# Experiment defintions: env, agent, controls, episodes
controls = np.linspace(0,7,num_actions)
episodes_train = 1000000
episodes_valid = 1000
reward_training = np.zeros((episodes_train, xi_.shape[0], disc1.shape[0], disc2.shape[0]))
reward_validation = np.zeros((episodes_valid, xi_.shape[0], disc1.shape[0], disc2.shape[0]))


#saving experimental conditions
df_eparams = pd.DataFrame(np.array([[num_actions, x0[0], modulus[0], state_UB[0], steps_[0], alph[0], disc1[0], disc2[0]],
                                      [None, x0[1], modulus[1], state_UB[1], None, None,   disc1[1], disc2[0]], 
                                      [None, None, None, None, None, None,  disc1[2], disc2[0]],
                                      [None, None, None, None, None, None,  disc1[3], disc2[0]],
                                      [None, None, None, None, None, None,  disc1[4], None],
                                      [None, None, None, None, None, None, disc1[5] , None],
                                      [None, None, None, None, None, None, None , None],
                                      [None, None, None, None, None, None, None , None],
                                      [None, None, None, None, None, None, None , None],
                                      [None, None, None, None, None, None, None , None]]), columns = ['num_actions','initial conds', 'modulus', 'state_UB', 'steps', 'LR', 'Discount1', 'Discount2'])
"""
  [None, x0[1], modulus[1], state_UB[1], None, None, disc1[1] , disc2[1]], 
                                      [None, None, None, None, None, None, disc1[2] , disc2[2]],
                                      [None, None, None, None, None, None, disc1[3] , disc2[3]],
                                      [None, None, None, None, None, None, disc1[4] , disc2[4]],
                                      [None, None, None, None, None, None, disc1[5] , disc2[5]],
                                      [None, None, None, None, None, None, disc1[6] , disc2[6]],
                                      [None, None, None, None, None, None, disc1[7] , disc2[7]],
                                      [None, None, None, None, None, None, disc1[8] , disc2[8]],
                                      [None, None, None, None, None, None, disc1[9] , disc2[9]]])
"""
df_behaviour = pd.DataFrame(np.array([[xi_[0], 0.3, 0.1]]), columns = ['Behaviour_trial', 'Inverse_of_Rate_Parameter','Random_Floor'])      
                                      #[xi_[1], 0.2, 0.1], 
                                      #[xi_[2], 0.3, 0.1],
                                      #[xi_[3], 0.4, 0.1],
                                      #[xi_[4], 0.5, 0.1],
                                      #[xi_[5], 0.05, 0.1],
                                      #[xi_[6], 0.01, 0.1]]), 

df_eparams.to_excel("P:\\Documents\\Projects\\First Year\\RL-ContinuousBP\\MCarlo\\Table Look Up\\Multiple\\IncrementalMean_VariationalDeriv\\Discount_Exp\\experimentalparameters.xlsx",index = False)
df_behaviour.to_excel("P:\\Documents\\Projects\\First Year\\RL-ContinuousBP\\MCarlo\\Table Look Up\\Multiple\\IncrementalMean_VariationalDeriv\\Discount_Exp\\behaviourparameters.xlsx", index = False)

#running experiement
for i in range(0, xi_.shape[0]):
  for j in range(0, disc1.shape[0]):    #already completed indexes 0,1,2 
    for k in range(0,disc2.shape[0]): 
      #run training 
      env = Model_env(p, steps_, tf, x0, modulus)
      agent1 = greedye_MCL(num_actions, modulus, state_UB, disc1[j], disc2[k], steps_)
      experiment = Experiment(env, agent1, controls, episodes_train, xi_[i])               #Q learning dictionary reinitiated upon calling whole experiement again
      reward_training[:,i,j,k], d, site_log = experiment.simulation()
      agent2 = Greedye_MCLearned(num_actions, d, steps_)
      exp_done = Experiment_Done(env, agent2, controls, episodes_valid, i, j, k, 0)
      reward_validation[:,i,j,k] = exp_done.simulation()
      export = reward_training[:,i,j,k]
      export2 = reward_validation[:,i,j,k]
      df = pd.DataFrame(export[0:1000000])
      #df1 = pd.DataFrame(export[1000000:2000000])
      #df2 = pd.DataFrame(export[2000000:3000000])
      #df3 = pd.DataFrame(export[3000000:4000000])
      #df4 = pd.DataFrame(export[4000000:5000000])
      #df5 = pd.DataFrame(export[5000000:6000000])
      #df6 = pd.DataFrame(export[5000000:6000000])
      #df7 = pd.DataFrame(export[6000000:7000000])
      #df8 = pd.DataFrame(export[7000000:8000000])
      #df9 = pd.DataFrame(export[8000000:9000000])
      #df10 = pd.DataFrame(export[9000000:-1])
      df11 = pd.DataFrame(export2)
